# Dimensionality Reduction with PCA and LDA

در این پروژه، نحوه‌ی کاهش ابعاد داده با دو روش **PCA** و **LDA** توضیح داده می‌شود.  
هدف، کاهش تعداد ویژگی‌ها با حفظ بیشترین اطلاعات مفید از داده است.

---

## PCA (Principal Component Analysis)

**PCA یک روش بدون ناظر (Unsupervised)** برای کاهش ابعاد است که بر اساس واریانس داده‌ها عمل می‌کند.

### ایده اصلی
PCA جهت‌هایی را در فضای داده پیدا می‌کند که بیشترین پراکندگی (واریانس) را دارند و داده‌ها را روی این جهت‌ها نگاشت می‌کند.

### مراحل انجام:
1. مرکز کردن داده‌ها (میانگین صفر)
2. محاسبه‌ی ماتریس کوواریانس
3. محاسبه‌ی مقادیر ویژه و بردارهای ویژه
4. انتخاب بردارهای ویژه با بزرگ‌ترین مقدار ویژه
5. پروجکت کردن داده‌ها روی این بردارها

### نتیجه:
- کاهش تعداد ابعاد
- حفظ بیشترین واریانس داده
- عدم استفاده از برچسب‌ها

---

## LDA (Linear Discriminant Analysis)

**LDA یک روش با ناظر (Supervised)** برای کاهش ابعاد است که از اطلاعات کلاس‌ها استفاده می‌کند.

### ایده اصلی
LDA جهت‌هایی را پیدا می‌کند که فاصله‌ی بین کلاس‌ها را بیشینه و پراکندگی داخل کلاس‌ها را کمینه کند.

### مراحل انجام:
1. محاسبه‌ی میانگین هر کلاس
2. محاسبه‌ی پراکندگی داخل کلاس‌ها (Within-class scatter)
3. محاسبه‌ی پراکندگی بین کلاس‌ها (Between-class scatter)
4. یافتن بردارهایی که نسبت پراکندگی بین کلاس‌ها به داخل کلاس‌ها را بیشینه کنند
5. نگاشت داده‌ها روی این بردارها

### نتیجه:
- کاهش تعداد ابعاد
- جداسازی بهتر کلاس‌ها
- نیاز به برچسب داده‌ها

---

## مقایسه PCA و LDA

| ویژگی | PCA | LDA |
|------|-----|-----|
| نوع روش | بدون ناظر | با ناظر |
| معیار کاهش بعد | بیشترین واریانس | بیشترین جداسازی کلاس‌ها |
| استفاده از برچسب | خیر | بله |
| حداکثر ابعاد خروجی | وابسته به داده | تعداد کلاس‌ها - 1 |

---

## جمع‌بندی

PCA برای کاهش بعد کلی داده و حذف نویز مناسب است،  
در حالی که LDA برای مسائلی که تفکیک کلاس‌ها اهمیت دارد عملکرد بهتری دارد.
